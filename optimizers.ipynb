{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: [ 4.00000010e-01 -2.00000005e-01  7.00000003e-01 -9.99999898e-09]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class Adagrad:\n",
    "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.cache = None # Will hold the sum of the squares of the gradients\n",
    "    def update(self, weights, gradients):\n",
    "        \"\"\"\n",
    "        Update weights using Adagrad algorithm\n",
    "        \n",
    "        weights: current values of model parameters (numpy array)\n",
    "        gradients: gradients of the loss w.r.t the weights (numpy array)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initilaize cache if it is None\n",
    "        if self.cache is None:\n",
    "            self.cache = np.zeros_like(weights)\n",
    "        \n",
    "        # Accumulate the squared gradients\n",
    "        self.cache += gradients**2\n",
    "\n",
    "        # Update weights\n",
    "        weights -= self.learning_rate * gradients / (np.sqrt(self.cache) + self.epsilon)\n",
    "\n",
    "        return weights\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        # Example usage\n",
    "    # Initial weights (parameters of a simple model)\n",
    "    weights = np.array([0.5, -0.3, 0.8, -0.1])  # Example weight vector\n",
    "\n",
    "    # Simulated gradients (gradients of the loss w.r.t these weights)\n",
    "    gradients = np.array([0.1, -0.2, 0.3, -0.1])  # Example gradient vector\n",
    "\n",
    "    # Initialize Adagrad optimizer\n",
    "    adagrad_optimizer = Adagrad(learning_rate=0.1)\n",
    "\n",
    "    # Perform a weight update using the gradients\n",
    "    updated_weights = adagrad_optimizer.update(weights, gradients)\n",
    "\n",
    "    print(\"Updated weights:\", updated_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 6.1367, Params = [0.45778736 0.66078453]\n",
      "Epoch 2: Loss = 5.7277, Params = [0.52612285 0.73002019]\n",
      "Epoch 3: Loss = 5.4105, Params = [0.58070821 0.78580217]\n",
      "Epoch 4: Loss = 5.1472, Params = [0.62715929 0.8336065 ]\n",
      "Epoch 5: Loss = 4.9204, Params = [0.66807876 0.8759813 ]\n",
      "Epoch 6: Loss = 4.7203, Params = [0.70492123 0.914354  ]\n",
      "Epoch 7: Loss = 4.5408, Params = [0.7385964  0.94961856]\n",
      "Epoch 8: Loss = 4.3778, Params = [0.76971778 0.98237843]\n",
      "Epoch 9: Loss = 4.2285, Params = [0.79872223 1.01306356]\n",
      "Epoch 10: Loss = 4.0905, Params = [0.82593388 1.04199308]\n",
      "Epoch 11: Loss = 3.9624, Params = [0.8516011  1.06941156]\n",
      "Epoch 12: Loss = 3.8427, Params = [0.87591919 1.09551132]\n",
      "Epoch 13: Loss = 3.7305, Params = [0.89904505 1.12044684]\n",
      "Epoch 14: Loss = 3.6249, Params = [0.92110696 1.14434445]\n",
      "Epoch 15: Loss = 3.5252, Params = [0.94221145 1.16730898]\n",
      "Epoch 16: Loss = 3.4308, Params = [0.96244813 1.18942861]\n",
      "Epoch 17: Loss = 3.3412, Params = [0.98189323 1.21077832]\n",
      "Epoch 18: Loss = 3.2559, Params = [1.00061226 1.23142252]\n",
      "Epoch 19: Loss = 3.1747, Params = [1.01866203 1.25141702]\n",
      "Epoch 20: Loss = 3.0971, Params = [1.03609215 1.27081056]\n",
      "Epoch 21: Loss = 3.0229, Params = [1.05294631 1.289646  ]\n",
      "Epoch 22: Loss = 2.9519, Params = [1.06926317 1.30796129]\n",
      "Epoch 23: Loss = 2.8838, Params = [1.08507718 1.32579018]\n",
      "Epoch 24: Loss = 2.8184, Params = [1.1004192  1.34316289]\n",
      "Epoch 25: Loss = 2.7555, Params = [1.11531697 1.36010659]\n",
      "Epoch 26: Loss = 2.6950, Params = [1.12979558 1.37664579]\n",
      "Epoch 27: Loss = 2.6368, Params = [1.14387779 1.39280274]\n",
      "Epoch 28: Loss = 2.5806, Params = [1.15758434 1.40859769]\n",
      "Epoch 29: Loss = 2.5264, Params = [1.17093417 1.42404909]\n",
      "Epoch 30: Loss = 2.4741, Params = [1.18394467 1.4391739 ]\n",
      "Epoch 31: Loss = 2.4236, Params = [1.19663181 1.45398766]\n",
      "Epoch 32: Loss = 2.3747, Params = [1.20901035 1.46850471]\n",
      "Epoch 33: Loss = 2.3274, Params = [1.22109393 1.48273829]\n",
      "Epoch 34: Loss = 2.2816, Params = [1.23289519 1.49670067]\n",
      "Epoch 35: Loss = 2.2372, Params = [1.24442588 1.51040323]\n",
      "Epoch 36: Loss = 2.1942, Params = [1.25569696 1.52385658]\n",
      "Epoch 37: Loss = 2.1525, Params = [1.26671865 1.53707057]\n",
      "Epoch 38: Loss = 2.1121, Params = [1.27750049 1.55005445]\n",
      "Epoch 39: Loss = 2.0728, Params = [1.28805145 1.56281683]\n",
      "Epoch 40: Loss = 2.0347, Params = [1.29837993 1.57536581]\n",
      "Epoch 41: Loss = 1.9976, Params = [1.30849385 1.58770899]\n",
      "Epoch 42: Loss = 1.9616, Params = [1.31840064 1.59985351]\n",
      "Epoch 43: Loss = 1.9266, Params = [1.32810733 1.6118061 ]\n",
      "Epoch 44: Loss = 1.8926, Params = [1.33762054 1.62357311]\n",
      "Epoch 45: Loss = 1.8594, Params = [1.34694657 1.63516053]\n",
      "Epoch 46: Loss = 1.8272, Params = [1.35609135 1.64657404]\n",
      "Epoch 47: Loss = 1.7958, Params = [1.36506052 1.657819  ]\n",
      "Epoch 48: Loss = 1.7652, Params = [1.37385943 1.6689005 ]\n",
      "Epoch 49: Loss = 1.7354, Params = [1.38249317 1.67982337]\n",
      "Epoch 50: Loss = 1.7064, Params = [1.39096659 1.69059221]\n",
      "Epoch 51: Loss = 1.6780, Params = [1.3992843  1.70121138]\n",
      "Epoch 52: Loss = 1.6504, Params = [1.40745071 1.71168503]\n",
      "Epoch 53: Loss = 1.6235, Params = [1.41547001 1.72201714]\n",
      "Epoch 54: Loss = 1.5973, Params = [1.42334623 1.73221147]\n",
      "Epoch 55: Loss = 1.5716, Params = [1.4310832  1.74227164]\n",
      "Epoch 56: Loss = 1.5466, Params = [1.4386846 1.7522011]\n",
      "Epoch 57: Loss = 1.5222, Params = [1.44615396 1.76200314]\n",
      "Epoch 58: Loss = 1.4983, Params = [1.45349466 1.77168092]\n",
      "Epoch 59: Loss = 1.4751, Params = [1.46070994 1.78123747]\n",
      "Epoch 60: Loss = 1.4523, Params = [1.46780291 1.79067569]\n",
      "Epoch 61: Loss = 1.4301, Params = [1.47477658 1.79999836]\n",
      "Epoch 62: Loss = 1.4083, Params = [1.48163382 1.80920815]\n",
      "Epoch 63: Loss = 1.3871, Params = [1.4883774  1.81830762]\n",
      "Epoch 64: Loss = 1.3663, Params = [1.49501    1.82729925]\n",
      "Epoch 65: Loss = 1.3460, Params = [1.50153419 1.83618541]\n",
      "Epoch 66: Loss = 1.3262, Params = [1.50795245 1.84496838]\n",
      "Epoch 67: Loss = 1.3068, Params = [1.51426717 1.85365036]\n",
      "Epoch 68: Loss = 1.2878, Params = [1.52048068 1.86223348]\n",
      "Epoch 69: Loss = 1.2692, Params = [1.5265952  1.87071978]\n",
      "Epoch 70: Loss = 1.2510, Params = [1.5326129  1.87911122]\n",
      "Epoch 71: Loss = 1.2332, Params = [1.53853587 1.88740972]\n",
      "Epoch 72: Loss = 1.2158, Params = [1.54436612 1.89561712]\n",
      "Epoch 73: Loss = 1.1987, Params = [1.55010561 1.90373519]\n",
      "Epoch 74: Loss = 1.1820, Params = [1.55575625 1.91176566]\n",
      "Epoch 75: Loss = 1.1656, Params = [1.56131987 1.91971018]\n",
      "Epoch 76: Loss = 1.1496, Params = [1.56679824 1.92757036]\n",
      "Epoch 77: Loss = 1.1339, Params = [1.57219311 1.93534776]\n",
      "Epoch 78: Loss = 1.1185, Params = [1.57750614 1.9430439 ]\n",
      "Epoch 79: Loss = 1.1035, Params = [1.58273896 1.95066022]\n",
      "Epoch 80: Loss = 1.0887, Params = [1.58789316 1.95819816]\n",
      "Epoch 81: Loss = 1.0743, Params = [1.59297027 1.96565907]\n",
      "Epoch 82: Loss = 1.0601, Params = [1.59797179 1.97304431]\n",
      "Epoch 83: Loss = 1.0462, Params = [1.60289916 1.98035516]\n",
      "Epoch 84: Loss = 1.0325, Params = [1.6077538  1.98759288]\n",
      "Epoch 85: Loss = 1.0192, Params = [1.61253708 1.99475868]\n",
      "Epoch 86: Loss = 1.0061, Params = [1.61725034 2.00185377]\n",
      "Epoch 87: Loss = 0.9932, Params = [1.62189488 2.00887928]\n",
      "Epoch 88: Loss = 0.9806, Params = [1.62647198 2.01583635]\n",
      "Epoch 89: Loss = 0.9683, Params = [1.63098286 2.02272606]\n",
      "Epoch 90: Loss = 0.9562, Params = [1.63542873 2.02954947]\n",
      "Epoch 91: Loss = 0.9443, Params = [1.63981077 2.03630762]\n",
      "Epoch 92: Loss = 0.9326, Params = [1.64413012 2.04300151]\n",
      "Epoch 93: Loss = 0.9211, Params = [1.64838789 2.04963213]\n",
      "Epoch 94: Loss = 0.9099, Params = [1.65258518 2.05620042]\n",
      "Epoch 95: Loss = 0.8989, Params = [1.65672306 2.06270733]\n",
      "Epoch 96: Loss = 0.8881, Params = [1.66080255 2.06915374]\n",
      "Epoch 97: Loss = 0.8774, Params = [1.66482467 2.07554056]\n",
      "Epoch 98: Loss = 0.8670, Params = [1.66879042 2.08186865]\n",
      "Epoch 99: Loss = 0.8568, Params = [1.67270076 2.08813883]\n",
      "Epoch 100: Loss = 0.8467, Params = [1.67655664 2.09435195]\n",
      "Epoch 101: Loss = 0.8369, Params = [1.68035897 2.10050879]\n",
      "Epoch 102: Loss = 0.8272, Params = [1.68410867 2.10661015]\n",
      "Epoch 103: Loss = 0.8177, Params = [1.68780662 2.11265679]\n",
      "Epoch 104: Loss = 0.8083, Params = [1.69145368 2.11864945]\n",
      "Epoch 105: Loss = 0.7991, Params = [1.69505069 2.12458887]\n",
      "Epoch 106: Loss = 0.7901, Params = [1.69859849 2.13047576]\n",
      "Epoch 107: Loss = 0.7813, Params = [1.70209789 2.13631081]\n",
      "Epoch 108: Loss = 0.7726, Params = [1.70554966 2.14209472]\n",
      "Epoch 109: Loss = 0.7640, Params = [1.7089546  2.14782815]\n",
      "Epoch 110: Loss = 0.7556, Params = [1.71231346 2.15351175]\n",
      "Epoch 111: Loss = 0.7474, Params = [1.71562698 2.15914617]\n",
      "Epoch 112: Loss = 0.7393, Params = [1.71889589 2.16473202]\n",
      "Epoch 113: Loss = 0.7313, Params = [1.72212091 2.17026993]\n",
      "Epoch 114: Loss = 0.7235, Params = [1.72530274 2.17576048]\n",
      "Epoch 115: Loss = 0.7158, Params = [1.72844205 2.18120428]\n",
      "Epoch 116: Loss = 0.7083, Params = [1.73153953 2.18660189]\n",
      "Epoch 117: Loss = 0.7008, Params = [1.73459582 2.19195388]\n",
      "Epoch 118: Loss = 0.6935, Params = [1.73761158 2.19726081]\n",
      "Epoch 119: Loss = 0.6863, Params = [1.74058744 2.20252321]\n",
      "Epoch 120: Loss = 0.6793, Params = [1.74352402 2.20774161]\n",
      "Epoch 121: Loss = 0.6724, Params = [1.74642193 2.21291654]\n",
      "Epoch 122: Loss = 0.6655, Params = [1.74928176 2.21804852]\n",
      "Epoch 123: Loss = 0.6588, Params = [1.7521041  2.22313803]\n",
      "Epoch 124: Loss = 0.6522, Params = [1.75488953 2.22818557]\n",
      "Epoch 125: Loss = 0.6457, Params = [1.7576386  2.23319163]\n",
      "Epoch 126: Loss = 0.6394, Params = [1.76035188 2.23815668]\n",
      "Epoch 127: Loss = 0.6331, Params = [1.76302991 2.24308118]\n",
      "Epoch 128: Loss = 0.6269, Params = [1.76567321 2.24796559]\n",
      "Epoch 129: Loss = 0.6209, Params = [1.76828232 2.25281036]\n",
      "Epoch 130: Loss = 0.6149, Params = [1.77085775 2.25761593]\n",
      "Epoch 131: Loss = 0.6090, Params = [1.7734     2.26238274]\n",
      "Epoch 132: Loss = 0.6033, Params = [1.77590957 2.26711119]\n",
      "Epoch 133: Loss = 0.5976, Params = [1.77838694 2.27180173]\n",
      "Epoch 134: Loss = 0.5920, Params = [1.7808326  2.27645475]\n",
      "Epoch 135: Loss = 0.5865, Params = [1.78324702 2.28107065]\n",
      "Epoch 136: Loss = 0.5811, Params = [1.78563064 2.28564985]\n",
      "Epoch 137: Loss = 0.5758, Params = [1.78798394 2.29019271]\n",
      "Epoch 138: Loss = 0.5706, Params = [1.79030735 2.29469964]\n",
      "Epoch 139: Loss = 0.5654, Params = [1.79260132 2.299171  ]\n",
      "Epoch 140: Loss = 0.5603, Params = [1.79486627 2.30360716]\n",
      "Epoch 141: Loss = 0.5554, Params = [1.79710262 2.3080085 ]\n",
      "Epoch 142: Loss = 0.5505, Params = [1.7993108  2.31237537]\n",
      "Epoch 143: Loss = 0.5456, Params = [1.80149121 2.31670812]\n",
      "Epoch 144: Loss = 0.5409, Params = [1.80364425 2.3210071 ]\n",
      "Epoch 145: Loss = 0.5362, Params = [1.80577033 2.32527266]\n",
      "Epoch 146: Loss = 0.5316, Params = [1.80786981 2.32950512]\n",
      "Epoch 147: Loss = 0.5271, Params = [1.8099431  2.33370483]\n",
      "Epoch 148: Loss = 0.5226, Params = [1.81199056 2.3378721 ]\n",
      "Epoch 149: Loss = 0.5182, Params = [1.81401257 2.34200726]\n",
      "Epoch 150: Loss = 0.5139, Params = [1.81600948 2.34611063]\n",
      "Epoch 151: Loss = 0.5097, Params = [1.81798166 2.35018252]\n",
      "Epoch 152: Loss = 0.5055, Params = [1.81992945 2.35422323]\n",
      "Epoch 153: Loss = 0.5014, Params = [1.8218532  2.35823307]\n",
      "Epoch 154: Loss = 0.4973, Params = [1.82375326 2.36221233]\n",
      "Epoch 155: Loss = 0.4933, Params = [1.82562995 2.36616131]\n",
      "Epoch 156: Loss = 0.4894, Params = [1.82748361 2.37008031]\n",
      "Epoch 157: Loss = 0.4855, Params = [1.82931456 2.3739696 ]\n",
      "Epoch 158: Loss = 0.4817, Params = [1.83112312 2.37782946]\n",
      "Epoch 159: Loss = 0.4780, Params = [1.8329096  2.38166018]\n",
      "Epoch 160: Loss = 0.4743, Params = [1.83467431 2.38546203]\n",
      "Epoch 161: Loss = 0.4706, Params = [1.83641756 2.38923527]\n",
      "Epoch 162: Loss = 0.4671, Params = [1.83813964 2.39298018]\n",
      "Epoch 163: Loss = 0.4635, Params = [1.83984085 2.39669701]\n",
      "Epoch 164: Loss = 0.4601, Params = [1.84152147 2.40038602]\n",
      "Epoch 165: Loss = 0.4566, Params = [1.8431818  2.40404748]\n",
      "Epoch 166: Loss = 0.4533, Params = [1.84482211 2.40768162]\n",
      "Epoch 167: Loss = 0.4500, Params = [1.84644268 2.41128871]\n",
      "Epoch 168: Loss = 0.4467, Params = [1.84804378 2.41486898]\n",
      "Epoch 169: Loss = 0.4435, Params = [1.84962568 2.41842268]\n",
      "Epoch 170: Loss = 0.4403, Params = [1.85118864 2.42195005]\n",
      "Epoch 171: Loss = 0.4372, Params = [1.85273293 2.42545131]\n",
      "Epoch 172: Loss = 0.4341, Params = [1.85425879 2.42892671]\n",
      "Epoch 173: Loss = 0.4311, Params = [1.85576648 2.43237648]\n",
      "Epoch 174: Loss = 0.4281, Params = [1.85725624 2.43580083]\n",
      "Epoch 175: Loss = 0.4252, Params = [1.85872833 2.4392    ]\n",
      "Epoch 176: Loss = 0.4223, Params = [1.86018298 2.4425742 ]\n",
      "Epoch 177: Loss = 0.4194, Params = [1.86162042 2.44592366]\n",
      "Epoch 178: Loss = 0.4166, Params = [1.8630409  2.44924859]\n",
      "Epoch 179: Loss = 0.4138, Params = [1.86444463 2.4525492 ]\n",
      "Epoch 180: Loss = 0.4111, Params = [1.86583185 2.4558257 ]\n",
      "Epoch 181: Loss = 0.4084, Params = [1.86720278 2.4590783 ]\n",
      "Epoch 182: Loss = 0.4058, Params = [1.86855764 2.46230721]\n",
      "Epoch 183: Loss = 0.4032, Params = [1.86989663 2.46551263]\n",
      "Epoch 184: Loss = 0.4006, Params = [1.87121999 2.46869475]\n",
      "Epoch 185: Loss = 0.3981, Params = [1.87252791 2.47185379]\n",
      "Epoch 186: Loss = 0.3956, Params = [1.8738206  2.47498992]\n",
      "Epoch 187: Loss = 0.3932, Params = [1.87509826 2.47810335]\n",
      "Epoch 188: Loss = 0.3907, Params = [1.8763611  2.48119427]\n",
      "Epoch 189: Loss = 0.3884, Params = [1.87760932 2.48426286]\n",
      "Epoch 190: Loss = 0.3860, Params = [1.87884309 2.48730932]\n",
      "Epoch 191: Loss = 0.3837, Params = [1.88006263 2.49033382]\n",
      "Epoch 192: Loss = 0.3814, Params = [1.88126812 2.49333656]\n",
      "Epoch 193: Loss = 0.3792, Params = [1.88245973 2.4963177 ]\n",
      "Epoch 194: Loss = 0.3770, Params = [1.88363767 2.49927743]\n",
      "Epoch 195: Loss = 0.3748, Params = [1.88480209 2.50221593]\n",
      "Epoch 196: Loss = 0.3726, Params = [1.8859532  2.50513336]\n",
      "Epoch 197: Loss = 0.3705, Params = [1.88709115 2.50802991]\n",
      "Epoch 198: Loss = 0.3684, Params = [1.88821612 2.51090574]\n",
      "Epoch 199: Loss = 0.3664, Params = [1.88932829 2.51376102]\n",
      "Epoch 200: Loss = 0.3643, Params = [1.89042781 2.51659592]\n",
      "Epoch 201: Loss = 0.3623, Params = [1.89151487 2.5194106 ]\n",
      "Epoch 202: Loss = 0.3604, Params = [1.8925896  2.52220523]\n",
      "Epoch 203: Loss = 0.3584, Params = [1.89365219 2.52497996]\n",
      "Epoch 204: Loss = 0.3565, Params = [1.89470278 2.52773496]\n",
      "Epoch 205: Loss = 0.3546, Params = [1.89574154 2.53047039]\n",
      "Epoch 206: Loss = 0.3528, Params = [1.89676861 2.5331864 ]\n",
      "Epoch 207: Loss = 0.3509, Params = [1.89778415 2.53588315]\n",
      "Epoch 208: Loss = 0.3491, Params = [1.8987883  2.53856079]\n",
      "Epoch 209: Loss = 0.3474, Params = [1.89978122 2.54121947]\n",
      "Epoch 210: Loss = 0.3456, Params = [1.90076305 2.54385934]\n",
      "Epoch 211: Loss = 0.3439, Params = [1.90173393 2.54648055]\n",
      "Epoch 212: Loss = 0.3422, Params = [1.902694   2.54908325]\n",
      "Epoch 213: Loss = 0.3405, Params = [1.9036434  2.55166759]\n",
      "Epoch 214: Loss = 0.3388, Params = [1.90458227 2.5542337 ]\n",
      "Epoch 215: Loss = 0.3372, Params = [1.90551074 2.55678173]\n",
      "Epoch 216: Loss = 0.3356, Params = [1.90642894 2.55931182]\n",
      "Epoch 217: Loss = 0.3340, Params = [1.90733702 2.56182411]\n",
      "Epoch 218: Loss = 0.3325, Params = [1.90823509 2.56431874]\n",
      "Epoch 219: Loss = 0.3309, Params = [1.90912328 2.56679584]\n",
      "Epoch 220: Loss = 0.3294, Params = [1.91000173 2.56925556]\n",
      "Epoch 221: Loss = 0.3279, Params = [1.91087054 2.57169802]\n",
      "Epoch 222: Loss = 0.3264, Params = [1.91172985 2.57412337]\n",
      "Epoch 223: Loss = 0.3250, Params = [1.91257978 2.57653172]\n",
      "Epoch 224: Loss = 0.3235, Params = [1.91342044 2.57892321]\n",
      "Epoch 225: Loss = 0.3221, Params = [1.91425195 2.58129797]\n",
      "Epoch 226: Loss = 0.3207, Params = [1.91507443 2.58365613]\n",
      "Epoch 227: Loss = 0.3194, Params = [1.91588799 2.58599782]\n",
      "Epoch 228: Loss = 0.3180, Params = [1.91669274 2.58832315]\n",
      "Epoch 229: Loss = 0.3167, Params = [1.91748879 2.59063226]\n",
      "Epoch 230: Loss = 0.3153, Params = [1.91827626 2.59292527]\n",
      "Epoch 231: Loss = 0.3140, Params = [1.91905524 2.59520229]\n",
      "Epoch 232: Loss = 0.3128, Params = [1.91982585 2.59746346]\n",
      "Epoch 233: Loss = 0.3115, Params = [1.92058819 2.59970889]\n",
      "Epoch 234: Loss = 0.3103, Params = [1.92134236 2.60193869]\n",
      "Epoch 235: Loss = 0.3090, Params = [1.92208847 2.60415299]\n",
      "Epoch 236: Loss = 0.3078, Params = [1.92282661 2.60635191]\n",
      "Epoch 237: Loss = 0.3066, Params = [1.92355689 2.60853555]\n",
      "Epoch 238: Loss = 0.3055, Params = [1.9242794  2.61070403]\n",
      "Epoch 239: Loss = 0.3043, Params = [1.92499423 2.61285747]\n",
      "Epoch 240: Loss = 0.3032, Params = [1.92570149 2.61499598]\n",
      "Epoch 241: Loss = 0.3020, Params = [1.92640127 2.61711967]\n",
      "Epoch 242: Loss = 0.3009, Params = [1.92709365 2.61922864]\n",
      "Epoch 243: Loss = 0.2998, Params = [1.92777873 2.62132302]\n",
      "Epoch 244: Loss = 0.2987, Params = [1.9284566 2.6234029]\n",
      "Epoch 245: Loss = 0.2977, Params = [1.92912734 2.62546839]\n",
      "Epoch 246: Loss = 0.2966, Params = [1.92979105 2.62751961]\n",
      "Epoch 247: Loss = 0.2956, Params = [1.93044781 2.62955665]\n",
      "Epoch 248: Loss = 0.2946, Params = [1.93109771 2.63157962]\n",
      "Epoch 249: Loss = 0.2936, Params = [1.93174082 2.63358863]\n",
      "Epoch 250: Loss = 0.2926, Params = [1.93237724 2.63558377]\n",
      "Epoch 251: Loss = 0.2916, Params = [1.93300704 2.63756515]\n",
      "Epoch 252: Loss = 0.2906, Params = [1.9336303  2.63953287]\n",
      "Epoch 253: Loss = 0.2897, Params = [1.93424711 2.64148703]\n",
      "Epoch 254: Loss = 0.2887, Params = [1.93485754 2.64342773]\n",
      "Epoch 255: Loss = 0.2878, Params = [1.93546166 2.64535506]\n",
      "Epoch 256: Loss = 0.2869, Params = [1.93605956 2.64726913]\n",
      "Epoch 257: Loss = 0.2860, Params = [1.9366513  2.64917003]\n",
      "Epoch 258: Loss = 0.2851, Params = [1.93723698 2.65105785]\n",
      "Epoch 259: Loss = 0.2842, Params = [1.93781664 2.6529327 ]\n",
      "Epoch 260: Loss = 0.2834, Params = [1.93839038 2.65479466]\n",
      "Epoch 261: Loss = 0.2825, Params = [1.93895825 2.65664384]\n",
      "Epoch 262: Loss = 0.2817, Params = [1.93952033 2.65848031]\n",
      "Epoch 263: Loss = 0.2809, Params = [1.94007669 2.66030418]\n",
      "Epoch 264: Loss = 0.2800, Params = [1.9406274  2.66211553]\n",
      "Epoch 265: Loss = 0.2792, Params = [1.94117252 2.66391445]\n",
      "Epoch 266: Loss = 0.2785, Params = [1.94171212 2.66570105]\n",
      "Epoch 267: Loss = 0.2777, Params = [1.94224627 2.66747539]\n",
      "Epoch 268: Loss = 0.2769, Params = [1.94277503 2.66923758]\n",
      "Epoch 269: Loss = 0.2761, Params = [1.94329846 2.67098769]\n",
      "Epoch 270: Loss = 0.2754, Params = [1.94381663 2.67272582]\n",
      "Epoch 271: Loss = 0.2746, Params = [1.9443296  2.67445205]\n",
      "Epoch 272: Loss = 0.2739, Params = [1.94483743 2.67616647]\n",
      "Epoch 273: Loss = 0.2732, Params = [1.94534018 2.67786916]\n",
      "Epoch 274: Loss = 0.2725, Params = [1.94583791 2.6795602 ]\n",
      "Epoch 275: Loss = 0.2718, Params = [1.94633068 2.68123968]\n",
      "Epoch 276: Loss = 0.2711, Params = [1.94681855 2.68290768]\n",
      "Epoch 277: Loss = 0.2704, Params = [1.94730157 2.68456429]\n",
      "Epoch 278: Loss = 0.2697, Params = [1.9477798  2.68620958]\n",
      "Epoch 279: Loss = 0.2691, Params = [1.9482533  2.68784363]\n",
      "Epoch 280: Loss = 0.2684, Params = [1.94872213 2.68946652]\n",
      "Epoch 281: Loss = 0.2678, Params = [1.94918632 2.69107834]\n",
      "Epoch 282: Loss = 0.2671, Params = [1.94964595 2.69267916]\n",
      "Epoch 283: Loss = 0.2665, Params = [1.95010107 2.69426906]\n",
      "Epoch 284: Loss = 0.2659, Params = [1.95055172 2.69584812]\n",
      "Epoch 285: Loss = 0.2653, Params = [1.95099795 2.69741641]\n",
      "Epoch 286: Loss = 0.2647, Params = [1.95143982 2.69897401]\n",
      "Epoch 287: Loss = 0.2641, Params = [1.95187738 2.700521  ]\n",
      "Epoch 288: Loss = 0.2635, Params = [1.95231068 2.70205744]\n",
      "Epoch 289: Loss = 0.2629, Params = [1.95273976 2.70358342]\n",
      "Epoch 290: Loss = 0.2624, Params = [1.95316468 2.70509901]\n",
      "Epoch 291: Loss = 0.2618, Params = [1.95358548 2.70660428]\n",
      "Epoch 292: Loss = 0.2612, Params = [1.95400221 2.70809931]\n",
      "Epoch 293: Loss = 0.2607, Params = [1.95441492 2.70958416]\n",
      "Epoch 294: Loss = 0.2602, Params = [1.95482364 2.71105891]\n",
      "Epoch 295: Loss = 0.2596, Params = [1.95522844 2.71252363]\n",
      "Epoch 296: Loss = 0.2591, Params = [1.95562934 2.71397838]\n",
      "Epoch 297: Loss = 0.2586, Params = [1.9560264  2.71542325]\n",
      "Epoch 298: Loss = 0.2581, Params = [1.95641965 2.7168583 ]\n",
      "Epoch 299: Loss = 0.2576, Params = [1.95680915 2.71828359]\n",
      "Epoch 300: Loss = 0.2571, Params = [1.95719493 2.71969919]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdagradOptimizer:\n",
    "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.grad_squared = None\n",
    "\n",
    "    def update(self, gradients, params):\n",
    "        if self.grad_squared is None:\n",
    "            self.grad_squared = [np.zeros_like(grad) for grad in gradients]\n",
    "\n",
    "        for i, (grad, param) in enumerate(zip(gradients, params)):\n",
    "            self.grad_squared[i] += grad ** 2\n",
    "            adjusted_lr = self.learning_rate / (np.sqrt(self.grad_squared[i]) + self.epsilon)\n",
    "            params[i] -= adjusted_lr * grad\n",
    "        return params\n",
    "\n",
    "# Linear Regression using Adagrad\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 1)\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(100) * 0.5\n",
    "\n",
    "# Add bias term to input\n",
    "X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "# Initialize parameters\n",
    "params = np.random.randn(2)\n",
    "optimizer = AdagradOptimizer(learning_rate=0.1)\n",
    "\n",
    "# Loss function (Mean Squared Error)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Gradient computation\n",
    "def compute_gradients(X, y, params):\n",
    "    y_pred = X @ params\n",
    "    error = y_pred - y\n",
    "    gradients = 2 * X.T @ error / len(y)\n",
    "    return gradients\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    gradients = compute_gradients(X_bias, y, params)\n",
    "    params = optimizer.update([gradients], [params])[0]\n",
    "    loss = mse_loss(y, X_bias @ params)\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {loss:.4f}, Params = {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.396593322390995"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2] @ params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
